# =================== 512 set ==================== #
caption_model: transformer
noamopt: true
noamopt_warmup: 20000
label_smoothing: 0.0
input_json: data/data_daisi/daisi_afterprepro.json
input_label_h5: data/data_daisi/daisi_afterprepro_label.h5
# input_att_dir: data/data_daisi/daisi_att
input_att_dir: data/data_daisi/daisi_resnet18_feat512_att

seq_per_img: 1
 #####################

batch_size: 9 #15 #75 #128 #128 # 256                                                 
learning_rate: 0.0005

# Notice: because I'm to lazy, I reuse the option name for RNNs to set the hyperparameters for transformer:
# N=num_layers
# d_model=input_encoding_size
# d_ff=rnn_size


num_layers: 6
input_encoding_size: 512 # input_encoding_size = d_model is always 512
rnn_size: 2048 #2048 # 2048 mengya: change 2048 into 512

fc_feat_size: 512
att_feat_size: 512

# Transformer config (just let you know)
N_enc: 6
N_dec: 6
d_model: 512
d_ff: 2048 # 2048 # 2048 mengya: change 2048 into 512
num_att_heads: 8
dropout: 0.1


learning_rate_decay_start: 0 # at what iteration to start decaying learning rate? (in epoch). decaying learning rate at epoch 0
scheduled_sampling_start: -1 # at what iteration to start decay gt probability？ (-1 = dont) (in epoch)
# save_checkpoint_every: 3000 # 'how often to save a model checkpoint (in iterations)?'
save_every_epoch: 1
# checkpoint_path: best_checkpoints/daisi

language_eval: 1
val_images_use: 1646
max_epochs: 100
train_sample_n: 1 ############# 'The reward weight from cider' # Used for self critical or structure. Used when sampling is need during training

REFORWARD: false

# 好像就算在 512 set 里面， 只需要 att_feat_size = 512， fc_feat_size = 512， d_ff/rnn_size等于2048，不等于512也是可以的